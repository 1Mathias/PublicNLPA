 # from nltk.tokenize import  sent_tokenize,word_tokenize
 # Example_word = "this could be us but no one cares"
 # print(word_tokenize(Example_word))
from nltk.tokenize import sent_tokenize,word_tokenize
Exmple_word = "This could be us! I am here to listen."
print(word_tokenize(Exmple_word))






